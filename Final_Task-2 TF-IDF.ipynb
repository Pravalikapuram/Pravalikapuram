{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453b69c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2756dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def Idf(documents,unique_words):\n",
    "    N=len(documents)\n",
    "    unique_words = set(' '.join(documents).split(\" \"))\n",
    "    wordToMatchingDocuments = Counter()\n",
    "    for document in documents:\n",
    "        wordToMatchingDocuments += Counter(set(document.split()))\n",
    "    idf = {k: math.log((N+1)/(v+1))+1 for k, v in wordToMatchingDocuments.items()}\n",
    "    top_idf_values=(sorted(idf.items(),key=lambda idf:(idf[1],idf[0]),reverse=True))\n",
    "    top_50_idf_values=top_idf_values[:50]\n",
    "    final_idf_values=dict(top_50_idf_values)\n",
    "    return final_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b460178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm # tqdm is a library that helps us to visualize the runtime of for loop. refer this to know more about tqdm\n",
    "#https://tqdm.github.io/\n",
    "\n",
    "# it accepts only list of sentances\n",
    "\n",
    "def fit(documents):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if isinstance(documents, (list,)):\n",
    "        for row in documents: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)} \n",
    "        Idf_all_unique_values=Idf(documents,unique_words)\n",
    "        top_elements=(sorted(Idf_all_unique_values.items(),key=lambda idf:(idf[1],idf[0])))\n",
    "        top_elements_vocab=top_elements[:50]\n",
    "        Top_50_vocab=dict(top_elements_vocab)\n",
    "        Top_50_vocab={key:value for value,key in enumerate(Idf_all_unique_values) }\n",
    "        \n",
    "        \n",
    "        \n",
    "        return Top_50_vocab,Idf_all_unique_values\n",
    "    else:\n",
    "        print(\"you need to pass list of sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4211f318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['zombiez', 'zillion', 'z', 'yun', 'youtube', 'youthful', 'younger', 'yelps', 'yawn', 'yardley', 'x', 'wrote', 'writers', 'wrap', 'wow', 'woven', 'wouldnt', 'worthy', 'worthwhile', 'worthless', 'worry', 'worked', 'woo', 'wont', 'wong', 'wondered', 'woa', 'witticisms', 'within', 'wise', 'win', 'wily', 'willie', 'william', 'wild', 'wih', 'wife', 'widmark', 'wide', 'whoever', 'whites', 'whine', 'whenever', 'went', 'welsh', 'weight', 'wedding', 'website', 'weaving', 'weariness'])\n",
      "dict_items([('zombiez', 6.922918004572872), ('zillion', 6.922918004572872), ('z', 6.922918004572872), ('yun', 6.922918004572872), ('youtube', 6.922918004572872), ('youthful', 6.922918004572872), ('younger', 6.922918004572872), ('yelps', 6.922918004572872), ('yawn', 6.922918004572872), ('yardley', 6.922918004572872), ('x', 6.922918004572872), ('wrote', 6.922918004572872), ('writers', 6.922918004572872), ('wrap', 6.922918004572872), ('wow', 6.922918004572872), ('woven', 6.922918004572872), ('wouldnt', 6.922918004572872), ('worthy', 6.922918004572872), ('worthwhile', 6.922918004572872), ('worthless', 6.922918004572872), ('worry', 6.922918004572872), ('worked', 6.922918004572872), ('woo', 6.922918004572872), ('wont', 6.922918004572872), ('wong', 6.922918004572872), ('wondered', 6.922918004572872), ('woa', 6.922918004572872), ('witticisms', 6.922918004572872), ('within', 6.922918004572872), ('wise', 6.922918004572872), ('win', 6.922918004572872), ('wily', 6.922918004572872), ('willie', 6.922918004572872), ('william', 6.922918004572872), ('wild', 6.922918004572872), ('wih', 6.922918004572872), ('wife', 6.922918004572872), ('widmark', 6.922918004572872), ('wide', 6.922918004572872), ('whoever', 6.922918004572872), ('whites', 6.922918004572872), ('whine', 6.922918004572872), ('whenever', 6.922918004572872), ('went', 6.922918004572872), ('welsh', 6.922918004572872), ('weight', 6.922918004572872), ('wedding', 6.922918004572872), ('website', 6.922918004572872), ('weaving', 6.922918004572872), ('weariness', 6.922918004572872)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "vocabulary,Idf_result=fit(corpus)\n",
    "print(vocabulary.keys())\n",
    "print(Idf_result.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7a0ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "def transform(dataset,vocabulary,idf_values):\n",
    "    if isinstance(dataset, (list,)):\n",
    "        sparse_matrix=csr_matrix((len(dataset),len(vocabulary)),dtype=np.float64)\n",
    "        #dataset = ' '.join(dataset).split()\n",
    "        for row in range (0, len(dataset)): # for each document in the dataset\n",
    "            word_freq =Counter(dataset[row].split())\n",
    "            for word in dataset[row].split():\n",
    "                    if word in list(vocabulary.keys()):\n",
    "                        tf_idf_values=(word_freq[word]/len(dataset[row].split()))*idf_values[word]\n",
    "                        #print(tf_idf_values)\n",
    "                        sparse_matrix[row,vocabulary[word]]=tf_idf_values\n",
    "                        #print(sparse_matrix)\n",
    "                        #print(sparse_matrix.shape)\n",
    "                        output=normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "                        \n",
    "               \n",
    "                    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d85c8e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 50)\n"
     ]
    }
   ],
   "source": [
    "final_output=transform(corpus,vocabulary,Idf_result)\n",
    "print(final_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "050a8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(final_output[].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98d9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
