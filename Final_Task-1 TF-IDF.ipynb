{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0945461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7acd876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "def Idf(documents,unique_words):\n",
    "    N=len(documents)\n",
    "    unique_words = set(' '.join(documents).split(\" \"))\n",
    "    wordToMatchingDocuments = Counter()\n",
    "    for document in documents:\n",
    "        wordToMatchingDocuments += Counter(set(document.split()))\n",
    "    idf = {k: math.log((N+1)/(v+1))+1 for k, v in wordToMatchingDocuments.items()}\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6f8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # tqdm is a library that helps us to visualize the runtime of for loop. refer this to know more about tqdm\n",
    "#https://tqdm.github.io/\n",
    "\n",
    "# it accepts only list of sentances\n",
    "\n",
    "def fit(documents):    \n",
    "    unique_words = set() # at first we will initialize an empty set\n",
    "    # check if its list type or not\n",
    "    if isinstance(documents, (list,)):\n",
    "        for row in documents: # for each review in the dataset\n",
    "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                unique_words.add(word)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_all_unique_values=Idf(documents,unique_words)\n",
    "        \n",
    "        return vocab,Idf_all_unique_values\n",
    "    else:\n",
    "        print(\"you need to pass list of sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5907b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'])\n",
      "dict_items([('document', 1.2231435513142097), ('is', 1.0), ('this', 1.0), ('first', 1.5108256237659907), ('the', 1.0), ('second', 1.916290731874155), ('one', 1.916290731874155), ('third', 1.916290731874155), ('and', 1.916290731874155)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]\n",
    "vocabulary,Idf_result=fit(corpus)\n",
    "print(vocabulary.keys())\n",
    "print(Idf_result.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e961ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "def transform(dataset,vocabulary,idf_values):\n",
    "    if isinstance(dataset, (list,)):\n",
    "        sparse_matrix=csr_matrix((len(dataset),len(vocabulary)),dtype=np.float64)\n",
    "        #dataset = ' '.join(dataset).split()\n",
    "        for row in range (0, len(dataset)): # for each document in the dataset\n",
    "            word_freq =Counter(dataset[row].split())\n",
    "            for word in dataset[row].split():\n",
    "                    if word in list(vocabulary.keys()):\n",
    "                        tf_idf_values=(word_freq[word]/len(dataset[row].split()))*idf_values[word]\n",
    "                        print(vocabulary[word])\n",
    "                        sparse_matrix[row,vocabulary[word]]=tf_idf_values\n",
    "                        output=normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "             \n",
    "                    \n",
    "        return output\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a0b89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "6\n",
      "2\n",
      "1\n",
      "8\n",
      "1\n",
      "3\n",
      "6\n",
      "5\n",
      "1\n",
      "0\n",
      "8\n",
      "3\n",
      "6\n",
      "7\n",
      "4\n",
      "3\n",
      "8\n",
      "6\n",
      "2\n",
      "1\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "final_output=transform(corpus,vocabulary,Idf_result)\n",
    "print(final_output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4dc4ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(final_output[0].toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
